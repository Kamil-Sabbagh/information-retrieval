{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words, concepts and search\n",
    "\n",
    "You are given a collection of texts, which for a dataset. Each string means a separate document. For each test we will answer 2 following questions:\n",
    "- Which of the documents is the closest to a given query?\n",
    "- How many concepts (i.e. principal components) is enough to represent these tests?\n",
    "\n",
    "Thus your result (answer) will consist of 2 integers, separated by a space: `doc_id concept_count`.\n",
    "\n",
    "## Let's consider the test example:\n",
    "\n",
    "`input.txt`\n",
    "\n",
    "```\n",
    "c d b.\n",
    "d e a.\n",
    "a b c.\n",
    "a b c d.\n",
    "d c a b.\n",
    "a c.      # <--- the last one is the query\n",
    "```\n",
    "\n",
    "## Let's do vectorization\n",
    "Reuse this in your solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['a', 'b', 'c', 'd', 'e']\n",
      "[[0.         0.57735027 0.57735027 0.57735027 0.        ]\n",
      " [0.440627   0.         0.         0.440627   0.78210977]\n",
      " [0.57735027 0.57735027 0.57735027 0.         0.        ]\n",
      " [0.5        0.5        0.5        0.5        0.        ]\n",
      " [0.5        0.5        0.5        0.5        0.        ]]\n",
      "\n",
      "Is it normed?\n",
      "\n",
      "[[1.         0.25439612 0.66666667 0.8660254  0.8660254 ]\n",
      " [0.25439612 1.         0.25439612 0.440627   0.440627  ]\n",
      " [0.66666667 0.25439612 1.         0.8660254  0.8660254 ]\n",
      " [0.8660254  0.440627   0.8660254  1.         1.        ]\n",
      " [0.8660254  0.440627   0.8660254  1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "inp = \"\"\"c d b.\n",
    "d e a.\n",
    "a b c.\n",
    "a b c d.\n",
    "d c a b.\n",
    "a c.\"\"\".split('\\n')\n",
    "\n",
    "dataset, query = inp[:-1], inp[-1]\n",
    "\n",
    "vect = TfidfVectorizer(\n",
    "            analyzer='word',\n",
    "            stop_words=None,\n",
    "            token_pattern=r\"(?u)\\b\\w+\\b\"    # (?u)\\b\\w\\w+\\b -- default pattern: (?u) -- unicode modifier, \\b -- word border, \\w\\w+ = 2+ letters\n",
    ")\n",
    "tdm = vect.fit_transform(dataset).todense()\n",
    "print(\"Vocabulary:\", vect.get_feature_names())\n",
    "print(tdm)\n",
    "\n",
    "print(\"\\nIs it normed?\\n\")\n",
    "print(tdm @ tdm.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, we are ready to answer the question 1. \n",
    "Which of the documents is the closest to a given query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarities of query and dataset:\n",
      " [[0.40824829]\n",
      " [0.31157034]\n",
      " [0.81649658]\n",
      " [0.70710678]\n",
      " [0.70710678]]\n",
      "Best match index: 2\n"
     ]
    }
   ],
   "source": [
    "# oops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time for question 2.\n",
    "How many concepts are enough to express our dataset?\n",
    "\n",
    "In other words, how many principal components do we need to preserve at least `0.95` of variance ratio?\n",
    "\n",
    "**NB: doing PCA you should think about how this method works! Can you just take the data and run PCA? Will it change the cosine metric? Can you do this without changing coordinate origin?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we take 0 components, variance = 0\n",
      "If we take 1 components, variance = 0.7506742465708932\n",
      "If we take 2 components, variance = 0.9205411757396144\n",
      "If we take 3 components, variance = 0.987207842406281\n",
      "If we take 4 components, variance = 1.0\n"
     ]
    }
   ],
   "source": [
    "# oops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarities of query and dataset (after reduction to 3 PC):\n",
      " [[0.41915156]\n",
      " [0.31571551]\n",
      " [0.82739985]\n",
      " [0.69640892]\n",
      " [0.69640892]]\n"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "# oops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost the same! We verified the job.\n",
    "\n",
    "### And the answer is...\n",
    "\n",
    "Thus, looking at the result of previous block, we can state:\n",
    "    \n",
    "`output.txt`\n",
    "```\n",
    "2 3\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
