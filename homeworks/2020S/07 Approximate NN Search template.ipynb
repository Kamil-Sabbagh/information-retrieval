{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate Nearest Neighbours Search\n",
    "\n",
    "Sometimes, when we are processing a user query, it may be **acceptable to retrieve a \"good guess\"** of \n",
    "nearest neighbours to the query **instead of true nearest neighbours**. In those cases, one can use an algorithm which doesn't guarantee to return the actual nearest neighbour in every case, **in return for improved speed or memory savings**. Thus, with the help of such algorithms one can do a **fast approximate search in a very large dataset**. Today we will expore one of such approaches based on graphs.\n",
    "\n",
    "This is what we are going to do in this lab: \n",
    "\n",
    "1. Complete implementation of small-world graph;\n",
    "2. Implement search in this graph;\n",
    "3. Build a *navigable* small-world graph;\n",
    "4. Compare search quality in the resulting graphs.\n",
    "\n",
    "\n",
    "## 1. Dataset preparation\n",
    "We will utilize the same dataset which was used in the last lab - the [dataset with curious facts](https://github.com/hsu-ai-course/hsu.ai/blob/master/code/datasets/nlp/facts.txt). Using trained `doc2vec` [model](https://github.com/jhlau/doc2vec) (Associated Press News DBOW (0.6GB), we will infer vectors for every fact and normalize them.\n",
    "\n",
    "\n",
    "### 1.1 Loading doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.doc2vec.Doc2Vec'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "# unpack a model into 3 files and target the main one\n",
    "# doc2vec.bin  <---------- this\n",
    "# doc2vec.bin.syn0.npy\n",
    "# doc2vec.bin.sin1neg.npy\n",
    "model = Doc2Vec.load('doc2vec.bin', mmap=None)\n",
    "print(type(model))\n",
    "print(type(model.infer_vector([\"to\", \"be\", \"or\", \"not\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "data_url = \"https://raw.githubusercontent.com/hsu-ai-course/hsu.ai/master/code/datasets/nlp/facts.txt\"\n",
    "file_name= \"facts.txt\"\n",
    "urllib.request.urlretrieve(data_url, file_name)\n",
    "\n",
    "facts = []\n",
    "with open(file_name) as fp:\n",
    "    for cnt, line in enumerate(fp):\n",
    "        facts.append(line.strip('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Transforming sentences into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def word_tokenize(sentence):\n",
    "    return nltk.word_tokenize(sentence.lower())\n",
    "\n",
    "def get_words_from_sentence(sentences):\n",
    "    for sentence in sentences: \n",
    "        yield word_tokenize(sentence.split('.', 1)[1])\n",
    "\n",
    "sent_vecs = np.array([])\n",
    "sent_vecs = np.array(list(model.infer_vector(words) for words in get_words_from_sentence(facts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Normalizing vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def norm_vectors(A):\n",
    "    An = A.copy()\n",
    "    for i, row in enumerate(An):\n",
    "        An[i, :] /= np.linalg.norm(row)\n",
    "    return An\n",
    "\n",
    "def find_k_closest(query, dataset, k=5):    \n",
    "    index = list((i, v, np.dot(query, v)) for i, v in enumerate(dataset))    \n",
    "    return sorted(index, key=lambda pair: pair[2], reverse=True)[:k]\n",
    "\n",
    "sent_vecs_normed = norm_vectors(sent_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Small world network ##\n",
    "We discussed [small world networks](https://en.wikipedia.org/wiki/Small-world_network) in lecture. This beautiful concept utilizes skip-list idea to reach query neighbourhood fastly from any random graph node. You have practically all code written, you just need to complete `rewire()` function with respect to [Watts–Strogatz process](https://en.wikipedia.org/wiki/Watts%E2%80%93Strogatz_model).\n",
    "\n",
    "**Please write rewiring code.**\n",
    "\n",
    "Function `build_graph` accepts some iterable collection of `values`. In our case this will be embeddings. \n",
    "\n",
    "- `K` is a parameter of Watts–Strogatz model, expressing average degree of graph nodes.\n",
    "- `p` stands for probability of \"rewiring\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "class Node:\n",
    "    ''' Graph node class. Major properties are `value` to access embedding and `neighbourhood` for adjacent nodes '''\n",
    "    def __init__(self, value, idx):\n",
    "        self.value = value\n",
    "        self.idx = idx\n",
    "        self.neighbourhood = set()\n",
    "        \n",
    "\n",
    "def build_graph(values, K, p=0.4):\n",
    "    '''Accepts container with values. Returns list with graph nodes'''\n",
    "    \n",
    "    def rewire(nodes, i, j, k):\n",
    "        #TODO remove i-j connection and add i-k connection, bi-directional\n",
    "        \n",
    "        return True\n",
    "    \n",
    "        \n",
    "    N = len(values)\n",
    "    nodes = [None] * N\n",
    "    \n",
    "    # create nodes\n",
    "    for i, val in enumerate(values):\n",
    "        nodes[i] = Node(val, i)\n",
    "    \n",
    "    # create K-regular lattice\n",
    "    for i, val in enumerate(nodes):\n",
    "        for j in range(i - K // 2, i + K // 2 + 1):\n",
    "            if i != j:\n",
    "                nodes[i].neighbourhood.add(j % N)\n",
    "                nodes[j % N].neighbourhood.add(i)\n",
    "        \n",
    "    for i, node in enumerate(nodes):\n",
    "        #TODO for each node rewire right hand side i-j edge to some other random node\n",
    "        # See Watts–Strogatz model for details\n",
    "        \n",
    "        pass\n",
    "                \n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bigger `K` and `p` you choose, the longer method runs. Bigger `K` leads to bigger near-cliques in a graph and, as a consequence, bigger context to consider at each step of search. Bigger `p` is for a better \"remote hops\", but it should not be close to 1, as it will make graph random (not SW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph built in 2.00 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "G = build_graph(sent_vecs_normed, K=10, p=0.2)\n",
    "finish = time.time()\n",
    "print(\"Graph built in {:.2f} ms\".format(1000 * (finish - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Searching in a small-world graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to implement an efficient search procedure which would utilize small world properties. Starting from the random node, at each step you should move towards the closest node (in terms of cosine simiarity, in our case), meanwhile keeping and refreshing top-K nearest neightbours collection. \n",
    "\n",
    "**Please implement basic NSW search**. \n",
    "\n",
    "You can refer to the `K-NNSearch` algorithm which pseudocode is given in section 4.2 of the [original paper](https://publications.hse.ru/mirror/pubs/share/folder/x5p6h7thif/direct/128296059).\n",
    "\n",
    "`search_nsw_basic()`\n",
    "- `query` - `vector` (`np.ndarray`) representing a query.\n",
    "- `nsw` - SW graph.\n",
    "- `top` - re-ranking set size.\n",
    "- `guard_hops` - if method does not converge, we will terminate when reaching guard_hops #steps.\n",
    "- `returns` - a pair of a `set` of indices and number of hops `(nearest_neighbours_set, hops)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import sortedcontainers\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def search_nsw_basic(query, nsw, top=5, guard_hops=100):\n",
    "    ''' basic search algorithm, takes vector query and returns a pair (nearest_neighbours, hops)'''\n",
    "    #TODO implement basic NSW search\n",
    "    \n",
    "    hops = 0    \n",
    "    return [], hops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Test the search procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "test_queries = [\"good mood\", \"birds\", \"virus and bacteria\"]\n",
    "test_vectors = np.array([model.infer_vector(word_tokenize(q)) for q in test_queries])\n",
    "test_queries_normed = norm_vectors(test_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's display the true nearest neighbours and measure average search time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for query: good mood\n",
      "\t 68. Cherophobia is the fear of fun. sim= 0.59210587\n",
      "\t 76. You breathe on average about 8,409,600 times a year sim= 0.5562223\n",
      "\t 144. Dolphins sleep with one eye open! sim= 0.5458862\n",
      "\t 97. 111,111,111 X 111,111,111 = 12,345,678,987,654,321 sim= 0.5400203\n",
      "\t 18. You cannot snore and dream at the same time. sim= 0.5364364\n",
      "\n",
      "Results for query: birds\n",
      "\t 47. Avocados are poisonous to birds. sim= 0.7138059\n",
      "\t 111. Butterflies taste their food with their feet. sim= 0.66970384\n",
      "\t 121. Birds don’t urinate. sim= 0.6401714\n",
      "\t 109. Cows kill more people than sharks do. sim= 0.6386259\n",
      "\t 144. Dolphins sleep with one eye open! sim= 0.6125039\n",
      "\n",
      "Results for query: virus and bacteria\n",
      "\t 47. Avocados are poisonous to birds. sim= 0.6077043\n",
      "\t 39. A 2010 study found that 48% of soda fountain contained fecal bacteria, and 11% contained E. Coli. sim= 0.6056562\n",
      "\t 54. Coconut water can be used as blood plasma. sim= 0.60471106\n",
      "\t 109. Cows kill more people than sharks do. sim= 0.5947235\n",
      "\t 83. During your lifetime, you will produce enough saliva to fill two swimming pools. sim= 0.5837047\n",
      "\n",
      "Exact search took 1.3322 ms on average\n"
     ]
    }
   ],
   "source": [
    "search_time = 0\n",
    "for i, query in enumerate(test_queries):\n",
    "    start = time.time()\n",
    "    r = find_k_closest(test_queries_normed[i], sent_vecs_normed)\n",
    "    finish = time.time()\n",
    "    search_time += finish - start  \n",
    "\n",
    "    print(\"\\nResults for query:\", query)\n",
    "    for k, v, p in r:\n",
    "        print(\"\\t\", facts[k], \"sim=\", p)\n",
    "\n",
    "print(\"\\nExact search took {:.4f} ms on average\".format(1000 * (search_time/len(test_queries))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see `search_nsw_basic` in action. It should work way faster than pairwise comparisons above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for query: good mood\n",
      "\t 76. You breathe on average about 8,409,600 times a year sim= 0.5562223\n",
      "\t 41. Blueberries will not ripen until they are picked. sim= 0.48288202\n",
      "\t 29. Chewing gum while you cut an onion will help keep you from crying. sim= 0.4531188\n",
      "\t 71. Human thigh bones are stronger than concrete. sim= 0.43797714\n",
      "\t 79. A waterfall in Hawaii goes up sometimes instead of down. sim= 0.41813037\n",
      "\n",
      "Results for query: birds\n",
      "\t 47. Avocados are poisonous to birds. sim= 0.7138059\n",
      "\t 44. Honey never spoils. sim= 0.5701026\n",
      "\t 49. The number of animals killed for meat every hour in the U.S. is 500,000. sim= 0.5694398\n",
      "\t 42. About 150 people per year are killed by coconuts. sim= 0.56624955\n",
      "\t 46. A hardboiled egg will spin, but a soft-boiled egg will not. sim= 0.52303654\n",
      "\n",
      "Results for query: virus and bacteria\n",
      "\t 39. A 2010 study found that 48% of soda fountain contained fecal bacteria, and 11% contained E. Coli. sim= 0.6056562\n",
      "\t 137. Human birth control pills work on gorillas. sim= 0.53696823\n",
      "\t 130. There are 60,000 miles of blood vessels in the human body. sim= 0.5194303\n",
      "\t 131. If a pregnant woman has organ damage, the baby in her womb sends stem cells to help repair the organ. sim= 0.50291526\n",
      "\t 133. Only one person in two billion will live to be 116 or older. sim= 0.45661914\n",
      "\n",
      "Basic nsw search took 0.6657 ms on average\n"
     ]
    }
   ],
   "source": [
    "search_time = 0\n",
    "for i, query in enumerate(test_queries):\n",
    "    start = time.time()\n",
    "    ans, hops = search_nsw_basic(test_queries_normed[i], G)\n",
    "    finish = time.time()\n",
    "    search_time += finish - start\n",
    "\n",
    "    print(\"\\nResults for query:\", query)\n",
    "    for k in ans:\n",
    "        print(\"\\t\", facts[k], \"sim=\", np.dot(test_queries_normed[i], sent_vecs_normed[k]))\n",
    "        \n",
    "print(\"\\nBasic nsw search took {:.4f} ms on average\".format(1000 * (search_time/len(test_queries))))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results you see should be worse than the exact nearest neighbours, however, not completely random. Pay attention to the similarity values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Navigable small-world graph\n",
    "\n",
    "When building small-world graph using Watts–Strogatz model, there was no notion of proximity between the nodes - it was completely ignored. In Navigable small-world graphs, however, the idea is to insert nodes in such a way that the cliques form real neighbourhoods, meaning points that are connected are close to each other. Please refer to section 5 of the [paper](https://publications.hse.ru/mirror/pubs/share/folder/x5p6h7thif/direct/128296059) for the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def build_navigable_graph(values, K):\n",
    "    '''Accepts container with values. Returns list with graph nodes.\n",
    "    K parameter stands for the size of the set of closest neighbors to connect to when adding a node'''\n",
    "    #TODO implement navigable small-world graph consrtuction  \n",
    "                \n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Building and testing the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "navigable_G = build_navigable_graph(sent_vecs_normed, K=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for query: good mood\n",
      "\t 68. Cherophobia is the fear of fun. sim= 0.59210587\n",
      "\t 45. About half of all Americans are on a diet on any given day. sim= 0.526565\n",
      "\t 57. Gorillas burp when they are happy sim= 0.5145933\n",
      "\t 70. Pirates wore earrings because they believed it improved their eyesight. sim= 0.49504918\n",
      "\t 6. There are more lifeforms living on your skin than there are people on the planet. sim= 0.48445895\n",
      "\n",
      "Results for query: birds\n",
      "\t 47. Avocados are poisonous to birds. sim= 0.7138059\n",
      "\t 111. Butterflies taste their food with their feet. sim= 0.66970384\n",
      "\t 12. A human will eat on average 70 assorted insects and 10 spiders while sleeping. sim= 0.593714\n",
      "\t 112. A tarantula can live without food for more than two years. sim= 0.5825779\n",
      "\t 110. Cats have 32 muscles in each of their ears. sim= 0.5541166\n",
      "\n",
      "Results for query: virus and bacteria\n",
      "\t 6. There are more lifeforms living on your skin than there are people on the planet. sim= 0.5208382\n",
      "\t 12. A human will eat on average 70 assorted insects and 10 spiders while sleeping. sim= 0.48884314\n",
      "\t 9. One in every five adults believe that aliens are hiding in our planet disguised as humans. sim= 0.46911395\n",
      "\t 16. Men are 6 times more likely to be struck by lightning than women. sim= 0.44280326\n",
      "\t 20. A coyote can hear a mouse moving underneath a foot of snow. sim= 0.43118066\n",
      "\n",
      "Search in navigable graph took 0.3335 ms on average\n"
     ]
    }
   ],
   "source": [
    "search_time = 0\n",
    "for i, query in enumerate(test_queries):\n",
    "    start = time.time()\n",
    "    ans, hops = search_nsw_basic(test_queries_normed[i], navigable_G) \n",
    "    finish = time.time()\n",
    "    search_time += finish - start\n",
    "    \n",
    "    print(\"\\nResults for query:\", query)\n",
    "    for k in ans:\n",
    "        print(\"\\t\", facts[k], \"sim=\", np.dot(test_queries_normed[i], sent_vecs_normed[k]))\n",
    "        \n",
    "print(\"\\nSearch in navigable graph took {:.4f} ms on average\".format(1000 * (search_time/len(test_queries))))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparing search quality in resulting graphs\n",
    "\n",
    "For both graphs, for each data sample, retrieve K nearest neighbours and compare them to the true nearest neighbours of the sample. If the retrieved result is present in the true top-k list of the sample, then it is counted as a hit. For both graphs, report the total number of hits, and the average number of hits per sample.\n",
    "\n",
    "For example: `Number of hits 394 out of 795, avg per query 2.48`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#TODO measure and report the described metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus task\n",
    "\n",
    "Generate a small set of 2d points and build 2 types of graphs for this set: small-world graph based on Watts–Strogatz algorithm, and Navigable small-world graph. Visualize both graphs and analyze their structures - do they differ? Does Navigable small-world graph capture geometric proximity better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#TODO bonus task"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
