{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building inverted index and answering queries\n",
    "\n",
    "In this lab you are going to implement a standard document processing pipeline and then build a simple search engine based on it:\n",
    "- starting from crawling documents, \n",
    "- then building an inverted index,\n",
    "- and answering queries using this index.\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "First, we need a unified approach to documents and queries preprocessing. Implement a class responsible for that. Complete the code for given functions (most of them are just one-liners) and make sure you pass the tests. Make use of `nltk` library, `spacy`, or any other you know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "class Preprocessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = { 'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', \n",
    "                            'for', 'from', 'has', 'he', 'in', 'is', 'it', 'its',\n",
    "                            'of', 'on', 'that', 'the', 'to', 'was', \n",
    "                            'were', 'will', 'with'}\n",
    "        self.ps = nltk.stem.PorterStemmer()\n",
    "\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        #TODO word tokenize text using nltk lib\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return ['one', 'two', 'three']\n",
    "\n",
    "    \n",
    "    def stem(self, word, stemmer):\n",
    "        #TODO stem word using provided stemmer\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return 'stemmed_word'\n",
    "\n",
    "    \n",
    "    def is_apt_word(self, word):\n",
    "        #TODO check if word is appropriate - not a stop word and isalpha, \n",
    "        # i.e consists of letters, not punctuation, numbers, dates\n",
    "        \n",
    "        # TODO YOUR CODE HERE\n",
    "        \n",
    "        return False\n",
    "\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        tokenized = self.tokenize(text.lower())\n",
    "        return [self.stem(w, self.ps) for w in tokenized if self.is_apt_word(w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = Preprocessor()\n",
    "text = 'To be, or not to be, that is the question'\n",
    "\n",
    "assert prep.tokenize(text) == ['To', 'be', ',', 'or', 'not', 'to', 'be', \n",
    "                               ',', 'that', 'is', 'the', 'question']\n",
    "assert prep.stem('retrieval', prep.ps) == 'retriev'\n",
    "assert prep.is_apt_word('qwerty123') is False\n",
    "assert prep.preprocess(text) == ['or', 'not', 'question']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling and Indexing\n",
    "\n",
    "### Base classes\n",
    "\n",
    "Here are some base classes you will need for writing an indexer. The code is given, still, you need to change it slightly. Namely, the `parse()` method. Add the code which will extract relevant text from the documents, particularly trageting Reuters website structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import quote\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib.parse\n",
    "import os\n",
    "\n",
    "\n",
    "class Document:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    def download(self):\n",
    "        try:\n",
    "            response = requests.get(self.url)\n",
    "            if response.status_code == 200:\n",
    "                self.content = response.content\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def persist(self, path):\n",
    "        # this code is not supposed to be good :) \n",
    "        # Please discuss why this line is bad\n",
    "        with open(os.path.join(path, quote(self.url).replace('/', '_')), 'wb') as f:\n",
    "            f.write(self.content)\n",
    "\n",
    "\n",
    "class HtmlReutersArticle(Document):\n",
    "\n",
    "    def normalize(self, href):\n",
    "        if href is not None and href[:4] != 'http':\n",
    "            href = urllib.parse.urljoin(self.url, href)\n",
    "        return href\n",
    "\n",
    "    def parse(self):\n",
    "        \n",
    "        def tag_visible(element):\n",
    "            if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "                return False\n",
    "            if isinstance(element, Comment):\n",
    "                return False\n",
    "            return True\n",
    "   \n",
    "        model = BeautifulSoup(self.content, \"html.parser\")\n",
    "        \n",
    "        self.anchors = []\n",
    "        a = model.find_all('a')\n",
    "        for anchor in a:\n",
    "            href = self.normalize(anchor.get('href'))\n",
    "            text = anchor.text\n",
    "            self.anchors.append((text, href))\n",
    "            \n",
    "        # extract only header and main content\n",
    "        # discuss why using classes like article-body__content__17Yit \n",
    "        # is the wrong strategy\n",
    "        header = # TODO your code here\n",
    "        content = # TODO your code here\n",
    "                        \n",
    "        if header is None or content is None:\n",
    "            self.article_text = \"\"\n",
    "            return\n",
    "        \n",
    "        texts = header.findAll(text=True) + content.findAll(text=True)\n",
    "        visible_texts = filter(tag_visible, texts)  \n",
    "        self.article_text = \"\\n\".join(t.strip() for t in visible_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Amazon launches virtual healthcare clinic in U.S. for common ailments\\nNov 15 (Reuters) - Amazon.com Inc\\n(AMZN.O)\\non Tuesday launched Amazon clinic, a virtual platform where users can connect with healthcare providers to help treat common ailments like allergies and skin conditions.\\nAmazon has for years sought to expand its presence in healthcare. It bought online pharmacy PillPack in 2018, underpinning a prescription delivery and price-comparison site it later launched as Amazon Pharmacy, which lets users buy over-the-counter drugs via Prime memberships.\\nAmazon\\nsaid\\nits new service would operate in 32 states, where customers who seek treatment, will be connected to healthcare providers. The service does not include health insurance and pricing will vary depending on providers, it added.\\nThe online retailer first piloted virtual care visits for its own staff in Seattle in 2019 before offering services to other employers under the Amazon Care brand, which it now plans to close down by the end of this year.\\nread more\\nThe company is also waiting to close its $3.49 billion deal to buy One Medical, as it seeks to expand its virtual healthcare presence and add brick-and-mortar doctors' offices for the first time.\\nread more\\nShares in Amazon were up about 1% in trading before the bell.\\nReporting by Bharat Govind Gautam, Eva Mathews and Manas Mishra in Bengaluru; Editing by Rashmi Aich and Dhanya Ann Thoppil\\nOur Standards:\\nThe Thomson Reuters Trust Principles.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = HtmlReutersArticle(\"https://www.reuters.com/business/healthcare-pharmaceuticals/amazon-launches-virtual-healthcare-clinic-us-2022-11-15/\")\n",
    "doc.download()\n",
    "doc.parse()\n",
    "doc.article_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main class\n",
    "\n",
    "The main indexer logic is here. We organize it as a crawler generator that adds certain visited pages to inverted index and saves them on disk. \n",
    "\n",
    "- `crawl_generator_for_index` method crawles the given website doing BFS, starting from `source` within given `depth`. Considers only inner pages (starting with https://www.reuters.com/...) for visiting. To speed up, do not consider pages with content type other than `html`: `.pdf`, `.mp3`, `.avi`, `.mp4`, `.txt`, ... . If crawler encounters an article page (of a form https://www.reuters.com/TOPIC/NAME...), it saves its content in a file in `collection_path` folder and populates the inverted index calling `index_doc` method. When done, saves on disk three resulting dictionaries:\n",
    "    - `doc_urls`: `{doc_id : url}`\n",
    "    - `index`: `{term : [collection_frequency, (doc_id_1, doc_freq_1), (doc_id_2, doc_freq_2), ...]}`\n",
    "    - `doc_lengths`: `{doc_id : doc_length}` \n",
    "\n",
    "    `limit` parameter is given for testing - if not `None`, break the loop when number of saved articles exceeds the `limit` and return without writing dictionaries to disk.\n",
    "    \n",
    "- `index_doc` method parses and preprocesses the content of a `doc` and adds it to the inverted index. Also keeps track of document lengths in a `doc_lengths` dictionary.\n",
    "\n",
    "\n",
    "**TODO**: at line 38 we have a regular expression that matches for https://www.reuters.com/TOPIC/ARTICLE-NAME links, but fails for topic pages and external websites. Please write this regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from queue import Queue\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "class ReutersSpecificIndexer:\n",
    "\n",
    "    def __init__(self):      \n",
    "        # dictionaries to populate\n",
    "        self.doc_urls = {}        \n",
    "        self.index = {}\n",
    "        self.doc_lengths = {}\n",
    "        # preprocessor\n",
    "        self.prep = Preprocessor()\n",
    "        \n",
    "    \n",
    "    def crawl_generator_for_index(self, source, depth, collection_path=\"collection\", limit=None):        \n",
    "        #TODO generate url-s for visiting\n",
    "        \n",
    "        q = Queue()\n",
    "        q.put((source, 0))\n",
    "        visited = set()\n",
    "        doc_counter = 0\n",
    "        # creating a folder if needed\n",
    "        if not os.path.exists(collection_path):\n",
    "            os.makedirs(collection_path)\n",
    "        # doing a BFS\n",
    "        while not q.empty():\n",
    "            url, url_depth = q.get()\n",
    "            if url not in visited:\n",
    "                visited.add(url)\n",
    "                try:\n",
    "                    doc = HtmlReutersArticle(url)    # download and parse url\n",
    "                    if doc.download():\n",
    "                        doc.parse()\n",
    "                        # TODO PLEASE WRITE A CORRECT REGULAR EXPRESSION\n",
    "                        if re.match(r'...', url):\n",
    "                            doc.persist(collection_path)\n",
    "                            self.doc_urls[doc_counter] = url\n",
    "                            self.index_doc(doc, doc_counter)\n",
    "                            doc_counter += 1                        \n",
    "                            yield doc\n",
    "                            if limit is not None and doc_counter == limit:\n",
    "                                return\n",
    "                            \n",
    "                            # filter links, consider only inner html pages\n",
    "                        if url_depth + 1 < depth:\n",
    "                            valid_anchors = filter(self.accepted_url, doc.anchors)\n",
    "                            for a in valid_anchors:\n",
    "                                q.put((a[1], url_depth + 1))\n",
    "    \n",
    "                except FileNotFoundError as e:\n",
    "                    print(\"Analyzing\", url, \"led to FileNotFoundError\")\n",
    "                    \n",
    "    \n",
    "    def accepted_url(self, anchor):\n",
    "        url = str(anchor[1])\n",
    "        if not url.startswith(\"https://www.reuters.com/\"):\n",
    "            return False\n",
    "        if url[-4:]  in ('.pdf', '.mp3', '.avi', '.mp4', '.txt'):\n",
    "            return False\n",
    "        return True\n",
    "        \n",
    "        \n",
    "    def index_doc(self, doc, doc_id):\n",
    "        # add documents to index\n",
    "        doc.parse()\n",
    "        # preprocess - tokenize, remove stopwords and non-alphanumeric tokens and stem\n",
    "        content = self.prep.preprocess(doc.article_text)\n",
    "        self.doc_lengths[doc_id] = len(content)\n",
    "        # get dict of terms in current article\n",
    "        article_index = Counter(content)\n",
    "        # update global index\n",
    "        for term in article_index.keys():\n",
    "            article_freq = article_index[term]\n",
    "            if term not in self.index:                \n",
    "                self.index[term] = [article_freq, (doc_id, article_freq)]\n",
    "            else:\n",
    "                self.index[term][0] += article_freq\n",
    "                self.index[term].append((doc_id, article_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests\n",
    "\n",
    "Please make sure your crawler prints out urls with `print(k, c.url)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = ReutersSpecificIndexer()\n",
    "k = 1\n",
    "for c in indexer.crawl_generator_for_index(\n",
    "        source=\"https://www.reuters.com/technology\", \n",
    "        depth=2, \n",
    "        collection_path=\"test_collection\", \n",
    "        limit=15):\n",
    "    print(k, c.url)\n",
    "    k += 1\n",
    "\n",
    "assert type(indexer.index) is dict\n",
    "assert type(indexer.index['reuter']) is list\n",
    "assert type(indexer.index['reuter'][0]) is int\n",
    "assert type(indexer.index['reuter'][1]) is tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please test these documents contain a desired stem (or its derivate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, (0, 2), (3, 1), (5, 1), (7, 1), (9, 1)]\n",
      "https://www.reuters.com/technology/ftx-officials-contact-with-us-regulators-filing-2022-11-15/\n",
      "https://www.reuters.com/markets/deals/buffetts-berkshire-discloses-big-taiwan-semi-stake-2022-11-14/\n",
      "https://www.reuters.com/technology/cryptoverse-so-long-solana-ether-rival-clobbered-by-ftx-crash-2022-11-15/\n",
      "https://www.reuters.com/business/finance/exclusive-activist-impactive-eyes-proxy-fight-envestnet-amid-sluggish-stock-2022-11-15/\n",
      "https://www.reuters.com/technology/japan-unicorn-opn-buys-merchante-enter-us-online-payment-sector-2022-11-15/\n"
     ]
    }
   ],
   "source": [
    "some_stem = 'bank'\n",
    "print(indexer.index[some_stem])\n",
    "for pair in indexer.index[some_stem][1:]:\n",
    "    print(indexer.doc_urls[pair[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4. Building an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = ReutersSpecificIndexer()\n",
    "for k, c in enumerate(\n",
    "                indexer\n",
    "                    .crawl_generator_for_index(\n",
    "                        \"https://www.reuters.com/\", \n",
    "                        3, \n",
    "                        \"docs_collection\",\n",
    "                        limit=100   # optional limit\n",
    "                    )):\n",
    "    print(k + 1, c.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index statistics\n",
    "\n",
    "Load the index and print the statistics. May you use this data to update stopwords list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total index keys count 4113\n",
      "\n",
      "Top stems by number of documents they apperared in:\n",
      "[('reuter', 61), ('trust', 56), ('thomson', 56), ('standard', 56), ('said', 56), ('principl', 56), ('our', 56), ('nov', 53), ('report', 52), ('edit', 48), ('have', 47), ('which', 43), ('not', 43), ('year', 42), ('had', 42), ('monday', 40), ('after', 40), ('more', 39), ('also', 39), ('their', 38)]\n",
      "\n",
      "Top stems by overall frequency:\n",
      "[('said', 336), ('reuter', 174), ('have', 131), ('not', 130), ('had', 115), ('which', 103), ('report', 100), ('more', 96), ('compani', 93), ('year', 88), ('firm', 87), ('ukrain', 84), ('after', 84), ('their', 81), ('china', 81), ('twitter', 72), ('than', 72), ('new', 72), ('law', 72), ('would', 71)]\n"
     ]
    }
   ],
   "source": [
    "print('Total index keys count', len(indexer.index))\n",
    "\n",
    "print('\\nTop stems by number of documents they apperared in:')\n",
    "sorted_by_n_docs = sorted(indexer.index.items(), key=lambda kv: (len(kv[1]), kv[0]), reverse=True)\n",
    "print([(sorted_by_n_docs[i][0], len(sorted_by_n_docs[i][1])) for i in range(20)])\n",
    "\n",
    "print('\\nTop stems by overall frequency:')\n",
    "sorted_by_freq = sorted(indexer.index.items(), key=lambda kv: (kv[1][0], kv[0]), reverse=True)\n",
    "print([(sorted_by_freq[i][0], sorted_by_freq[i][1][0]) for i in range(20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering a query (finally)\n",
    "\n",
    "Now, given that we already have built the inverted index, it's time to utilize it for answering user queries. In this class there are two methods you need to implement:\n",
    "- `boolean_retrieval`, the simplest form of document retrieval which returns a set of documents such that each one contains all query terms. Returns a set of document ids. Refer to *ch.1* of the book for details;\n",
    "- `okapi_scoring`, Okapi BM25 ranking function - assigns scores to documents in the collection that are relevant to the user query. Returns a dictionary of scores, `doc_id:score`. Read about it in [Wikipedia](https://en.wikipedia.org/wiki/Okapi_BM25#The_ranking_function) and implement accordingly.\n",
    "\n",
    "Both methods accept `query` parameter in a form of a dictionary, `term:frequency`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "class QueryProcessing:\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_query(raw_query):\n",
    "        prep = Preprocessor()\n",
    "        # pre-process query the same way as documents\n",
    "        query = prep.preprocess(raw_query)\n",
    "        # count frequency\n",
    "        return Counter(query)\n",
    "    \n",
    "    @staticmethod\n",
    "    def boolean_retrieval(query, index):\n",
    "        #TODO retrieve a set of documents containing all query terms\n",
    "        \n",
    "        # TODO YOUR CODE HERE\n",
    "        # retrieve a set of documents containing all query terms\n",
    "        \n",
    "        return {0, 1, 3}\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def okapi_scoring(query, doc_lengths, index, k1=1.2, b=0.75):\n",
    "        #TODO retrieve relevant documents with scores\n",
    "        \n",
    "        # TODO YOUR CODE HERE\n",
    "        \n",
    "        return {0: 0.32, 5: 1.17}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc_lengths = {1: 20, 2: 15, 3: 10, 4:20, 5:30}\n",
    "test_index = {'x': [2, (1, 1), (2, 1)], 'y': [2, (1, 1), (3, 1)], 'z': [3, (2, 1), (4,2)]}\n",
    "\n",
    "\n",
    "test_query1 = QueryProcessing.prepare_query('x z')\n",
    "test_query2 = QueryProcessing.prepare_query('x y')\n",
    "\n",
    "\n",
    "assert QueryProcessing.boolean_retrieval(test_query1, test_index) == {2}\n",
    "assert QueryProcessing.boolean_retrieval(test_query2, test_index) == {1}\n",
    "okapi_res = QueryProcessing.okapi_scoring(test_query2, test_doc_lengths, test_index)\n",
    "assert all(k in okapi_res for k in (1, 2, 3))\n",
    "assert not any(k in okapi_res for k in (4, 5))\n",
    "assert okapi_res[1] > okapi_res[3] > okapi_res[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now query the real index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "russia ukraine\n",
      "\t https://www.reuters.com/business/energy/china-refiners-slow-down-russian-oil-purchases-sanctions-near-trade-2022-11-14/\n",
      "\t https://www.reuters.com/world/g20-starts-bali-ukraine-war-raging-inflation-top-agenda-2022-11-15/\n",
      "\t https://www.reuters.com/world/asia-pacific/g20-summit-latest-biden-xi-meet-2022-11-14/\n",
      "\t https://www.reuters.com/world/ahead-tense-g20-summit-biden-xi-meet-talks-2022-11-14/\n",
      "\t https://www.reuters.com/world/most-g20-members-strongly-condemn-war-ukraine-draft-declaration-says-2022-11-15/\n",
      "\t https://www.reuters.com/world/inside-besieged-mariupol-left-ruins-after-russian-bombardment-2022-03-24/\n",
      "\t https://www.reuters.com/markets/global-markets-view-usa-2022-11-15/\n",
      "\t https://www.reuters.com/world/europe/why-stop-now-ukraine-seen-pressing-advantage-after-kherson-victory-2022-11-14/\n",
      "\t https://www.reuters.com/world/europe/ukraine-hails-chinas-opposition-nuclear-threats-2022-11-15/\n",
      "\t https://www.reuters.com/world/europe/\n",
      "\t https://www.reuters.com/world/europe/both-russia-ukraine-tortured-prisoners-war-un-says-2022-11-15/\n",
      "\t https://www.reuters.com/world/europe/ukraine-shipping-hub-cheers-kherson-win-foils-russian-black-sea-hopes-2022-11-15/\n",
      "\t == Okapi Time: 0.00008 ==\n",
      "\t https://www.reuters.com/world/g20-starts-bali-ukraine-war-raging-inflation-top-agenda-2022-11-15/ 2.8338674144680147\n",
      "\t https://www.reuters.com/world/asia-pacific/g20-summit-latest-biden-xi-meet-2022-11-14/ 2.365108640810145\n",
      "\t https://www.reuters.com/world/ahead-tense-g20-summit-biden-xi-meet-talks-2022-11-14/ 1.9448771884956169\n",
      "\t https://www.reuters.com/world/most-g20-members-strongly-condemn-war-ukraine-draft-declaration-says-2022-11-15/ 2.6188983960224914\n",
      "\t https://www.reuters.com/markets/global-markets-view-usa-2022-11-15/ 2.091708236266899\n",
      "\t https://www.reuters.com/world/europe/why-stop-now-ukraine-seen-pressing-advantage-after-kherson-victory-2022-11-14/ 2.5667716533330975\n",
      "\t https://www.reuters.com/markets/europe/euro-zone-trade-gap-smaller-than-expected-russia-deficit-surges-2022-11-15/ 1.226982732296561\n",
      "\t https://www.reuters.com/world/inside-besieged-mariupol-left-ruins-after-russian-bombardment-2022-03-24/ 2.441723743974326\n",
      "\t https://www.reuters.com/world/europe/ukraine-hails-chinas-opposition-nuclear-threats-2022-11-15/ 2.7572586111106383\n",
      "\t https://www.reuters.com/world/europe/ 2.4869173524854267\n",
      "\t https://www.reuters.com/world/middle-east/uae-official-calls-unambivalent-us-security-commitment-2022-11-14/ 0.7702604875357449\n",
      "\t https://www.reuters.com/world/europe/both-russia-ukraine-tortured-prisoners-war-un-says-2022-11-15/ 2.5659740624788507\n",
      "\t https://www.reuters.com/world/europe/ukraine-shipping-hub-cheers-kherson-win-foils-russian-black-sea-hopes-2022-11-15/ 2.660278935466974\n",
      "\t https://www.reuters.com/business/energy/china-refiners-slow-down-russian-oil-purchases-sanctions-near-trade-2022-11-14/ 1.209478047247175\n",
      "\t https://www.reuters.com/business/cop/rich-nations-stick-coal-phase-out-plans-china-builds-new-projects-2022-11-15/ 0.6232595631053137\n",
      "\t https://www.reuters.com/legal/no-criminal-charges-forthcoming-giuliani-probe-prosecutor-says-2022-11-14/ 1.1376730441019653\n",
      "\t https://www.reuters.com/world/china/ 1.101313831021442\n",
      "\n",
      "Hong Kong\n",
      "\t https://www.reuters.com/world/ahead-tense-g20-summit-biden-xi-meet-talks-2022-11-14/\n",
      "\t https://www.reuters.com/markets/deals/jdcom-fintech-unit-aims-win-beijing-approval-hk-ipo-soon-year-end-sources-2022-11-15/\n",
      "\t == Okapi Time: 0.00004 ==\n",
      "\t https://www.reuters.com/world/ahead-tense-g20-summit-biden-xi-meet-talks-2022-11-14/ 2.5765622491543643\n",
      "\t https://www.reuters.com/markets/deals/jdcom-fintech-unit-aims-win-beijing-approval-hk-ipo-soon-year-end-sources-2022-11-15/ 5.392114470663575\n",
      "\t https://www.reuters.com/technology/chinas-tencent-starts-new-round-layoffs-sources-2022-11-15/ 1.4556402349466695\n",
      "\n",
      "crypto money\n",
      "\t https://www.reuters.com/technology/ftxs-new-ceo-helped-bolster-enron-victims-recovery-2022-11-15/\n",
      "\t == Okapi Time: 0.00004 ==\n",
      "\t https://www.reuters.com/technology/ftxs-new-ceo-helped-bolster-enron-victims-recovery-2022-11-15/ 2.2452341984552673\n",
      "\t https://www.reuters.com/technology/cryptoverse-so-long-solana-ether-rival-clobbered-by-ftx-crash-2022-11-15/ 2.0183854482768098\n",
      "\t https://www.reuters.com/markets/global-markets-view-usa-2022-11-15/ 1.4401085437157044\n",
      "\t https://www.reuters.com/technology/ftx-officials-contact-with-us-regulators-filing-2022-11-15/ 2.1384302474573027\n",
      "\t https://www.reuters.com/technology/binance-voyager-crypto-firms-exposure-ftx-is-coming-light-2022-11-14/ 2.2940981842144845\n",
      "\t https://www.reuters.com/business/future-of-money/ 1.9878806274928549\n",
      "\t https://www.reuters.com/legal/elon-musk-trial-opens-decide-fate-his-56-bln-tesla-pay-2022-11-14/ 0.9873475589443363\n",
      "\t https://www.reuters.com/business/charged/ 1.5816381571190072\n",
      "\t https://www.reuters.com/business/cop/ 1.6035762383908856\n",
      "\t https://www.reuters.com/world/us/watchdog-asks-us-election-regulator-probe-20-mln-trump-transfer-2022-11-14/ 1.7387394773609006\n",
      "\t https://www.reuters.com/legal/siebel-newsom-wife-california-governor-accuses-harvey-weinstein-rape-2022-11-15/ 0.8240511919616604\n",
      "\t https://www.reuters.com/legal/government/seven-years-after-ferguson-investigation-missouri-police-still-resist-reforms-2022-11-10/ 0.5191424655694921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "queries = [\"russia ukraine\", \"Hong Kong\", \"crypto money\"]\n",
    "for q in queries:\n",
    "    print(q)\n",
    "    qobj = QueryProcessing.prepare_query(q)\n",
    "    for res in QueryProcessing.boolean_retrieval(qobj, indexer.index):\n",
    "        print('\\t', indexer.doc_urls[res])\n",
    "\n",
    "    s = time.time()\n",
    "    okapi_res = QueryProcessing.okapi_scoring(qobj, indexer.doc_lengths, indexer.index)\n",
    "    e = time.time()\n",
    "    print(f\"\\t == Okapi Time: {e - s:.5f} ==\")\n",
    "    for res in okapi_res:\n",
    "        print('\\t', indexer.doc_urls[res], okapi_res[res])\n",
    "    \n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
