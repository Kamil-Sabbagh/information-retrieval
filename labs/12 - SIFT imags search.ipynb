{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. [50] Image search using SIFT\n",
    "\n",
    "Let's think about information retrieval in the context of image search. How can we find images similar to a query in a fast way (faster than doing pair-wise comparison with all images in a database)? How can we identify same objects taken in slightly different contexts? \n",
    "\n",
    "One way to do this is to find special points of interest in every image, so called keypoints (or descriptors), which characterize the image and which are more or less invariant to scaling, orientation, illumination changes, and some other distortions. There are several algorithms available that identify such keypoints, and today we will focus on [SIFT](https://en.wikipedia.org/wiki/Scale-invariant_feature_transform). \n",
    "\n",
    "Your task is to apply SIFT to a dataset of images and enable similar images search.\n",
    "\n",
    "## 1.1. Get dataset\n",
    "\n",
    "We will use `Caltech 101` dataset, download it from [here](http://www.vision.caltech.edu/Image_Datasets/Caltech101/). It consists of pictures of objects belonging to 101 categories. About 40 to 800 images per category. Most categories have about 50 images. The size of each image is roughly 300 x 200 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. SIFT example\n",
    "\n",
    "Below is an example how to extract SIFT keyponts using `opencv`. [This](https://docs.opencv.org/trunk/da/df5/tutorial_py_sift_intro.html) is a dedicated tutorial, and [this](https://docs.opencv.org/master/dc/dc3/tutorial_py_matcher.html) is another tutorial you may need to find matches between two images (use in your code `cv.drawMatches()` function to display keypoint matches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img_dir = '101_ObjectCategories'\n",
    "img = cv.imread(img_dir + '/gramophone/image_0018.jpg')\n",
    "gray= cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    "sift = cv.xfeatures2d.SIFT_create()\n",
    "kp = sift.detect(gray,None)\n",
    "img=cv.drawKeypoints(gray,kp,img,flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. [50] Index of keypoints\n",
    "\n",
    "Let's suppose we've found image descriptors. How do we find similar images, having this information? In our case the descriptors are 128-dinensional vectors per keypoint, and there can be hundreds of such points. To enable fast search of similar images, you will index descriptors of all images using some data structure for approximate nearest neighbors search, such as Navigable Small World or Annoy. Then, for a new (query) image you will generate its descriptors, and for each of them find its `k` nearest neighbors (using Euclidean or Cosine distance, which you prefer). Finally, you will sort potential similar images (retrieved from neighbor descriptors) by frequency with which they appear in k nearest neighbors (more matches -- higher the rank).\n",
    "\n",
    "### 1.3.1. [30] Build an index\n",
    "\n",
    "Read all images, saving category information. For every image generate SIFT descriptors and index them using HNSW from [`nmslib`](https://github.com/nmslib/nmslib), FAISS, Annoy or whatever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO do your job here\n",
    "## key is a descriptor, value is a filename\n",
    "index = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. [20] Implement search function\n",
    "\n",
    "Implement a function which returns `k` neighbours (names) sorted for a given image name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anns(imagename, k):\n",
    "    ## ... do your job here\n",
    "    similarity = -1\n",
    "    filename = 'strawberry/image_0022.jpg'\n",
    "    return sorted([(similarity, filename)])\n",
    "\n",
    "\n",
    "# finds query image in the result, as it is indexed\n",
    "filename = 'strawberry/image_0022.jpg'\n",
    "assert any([f[1] == filename for f in anns(filename, 10)]), \"Should return a duplicate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.3. [Extra] Estimate quality\n",
    "\n",
    "Build a bucket from these images.\n",
    "```\n",
    "accordion/image_0043.jpg\n",
    "laptop/image_0052.jpg\n",
    "pagoda/image_0038.jpg\n",
    "revolver/image_0043.jpg\n",
    "rhino/image_0040.jpg\n",
    "sea_horse/image_0038.jpg\n",
    "soccer_ball/image_0057.jpg\n",
    "starfish/image_0011.jpg\n",
    "strawberry/image_0022.jpg\n",
    "wrench/image_0013.jpg\n",
    "```\n",
    "Consider `relevant` if **class of the query and class of the result match**. Compute `DCG` and `pFound` for every query and for the bucket in average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
